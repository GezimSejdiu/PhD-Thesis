%==============================================================================
\chapter{Conclusion and Future Directions}
\label{chapter:conclusion}
%==============================================================================

In this chapter, we summarize the work done during this thesis and highlight the main results.
During this thesis, we studied the research problem of efficient distributed in-memory processing of \gls{RDF} datasets.

In particular, we addressed the problems of Scalable Computation of \gls{RDF} Dataset Statistics (cf. Chapter~\ref{chapter:dist_lod_stats}), Quality Assessment of \gls{RDF} Datasets at Scale (cf. Chapter~\ref{chapter:dist_quality_assessment}), Scalable and Efficient \gls{SPARQL} Query Evaluation (cf. Chapter~\ref{chapter:scalable_rdf_querying}), and usage of such scalable approaches into real-world use cases (cf. Chapter~\ref{chapter:implementation_and_use_cases}).

In the following sections, we provide a summary of our contributions and elaborate on the main findings that validate our research questions.

\section{Review of the Contributions}
In this section, we give an overview of the thesis' contributions in terms of the problems solved and how they offer concrete and valid solutions to the research questions.
The main goal of the thesis is to advance the area of distributed processing of \gls{RDF} datasets by providing a novel set of approaches in order to solve the main challenges in a distributed and scalable setting.
In this respect, our contributions answer four research questions.
Let us revisit the research questions defined during this thesis.

First, we tackled the problem of exploring the structure of the large-scale \gls{RDF} datasets and answering the following research question.

\begin{tcolorbox}
\textbf{\rqNr[RQ1]\label{rqc:1}}: How can we efficiently explore the structure of large-scale \gls{RDF} datasets?
\end{tcolorbox}

Over the last years, the Semantic Web has been growing steadily. 
Today, we count more than 10,000 datasets made available online following Semantic Web standards.
Nevertheless, many applications, such as data integration, search, and interlinking, may not take the full advantage of the data without having a priori statistical information about its internal structure and coverage.
In fact, there are already a number of tools, which offer such statistics, providing basic information about \gls{RDF} datasets and vocabularies.
However, those usually show severe deficiencies in terms of performance once the dataset size grows beyond the capabilities of a single machine.
To address~\ref{rqc:1}, in Chapter~\ref{chapter:dist_lod_stats} we introduced a software component for statistical calculations of large \gls{RDF} datasets, which scales out to clusters of machines.
More specifically, we described the first distributed in-memory approach for computing 32 different statistical criteria for \gls{RDF} datasets using Apache Spark.
The preliminary results show that our distributed approach improves upon a previous centralized approach we compare against and provides approximately linear horizontal scale-up. 
The criteria are extensible beyond the 32 default criteria, is integrated into the larger SANSA framework and employed in at least four major usage scenarios beyond the SANSA community.
Overall, we provide the following contributions to the state-of-the-art:

\begin{itemize}
    \item We proposed an algorithm for computing \gls{RDF} dataset statistics and implement it using an efficient framework for large-scale, distributed and in-memory computations: Apache Spark.
    \item We performed an analysis of the complexity of the computational steps and the data exchange between nodes in the cluster. 
    \item We evaluated our approach and demonstrate empirically its superiority over a previous centralized approach.
    \item We integrated the approach into the SANSA framework, where it is actively maintained and re-uses the community infrastructure (mailing list, issues trackers, website, etc.).
    \item An approach for triggering \gls{RDF} statistics calculation remotely simply using HTTP requests. 
    DistLODStats is built as a plugin into the larger SANSA framework and makes use of Apache Livy, a novel lightweight solution for interacting with the Spark cluster via a REST Interface.
\end{itemize}

The second problem we tried to address was the possibility of assessing the quality of large-scale \gls{RDF} datasets efficiently in a distributed manner and answers the following research question.

\begin{tcolorbox}
%\textbf{\rqNr[RQ2]\label{rqc:2}}: Can quality of large-scale \gls{RDF} datasets be assessed efficiently in a distributed manner?
\textbf{\rqNr[RQ2]\label{rqc:2}}: Can we scale \gls{RDF} dataset quality assessment horizontally?
\end{tcolorbox}

Over the last years, Linked Data has grown continuously. 
Today, we count more than 10,000 datasets being available online following Linked Data standards. 
These standards allow data to be machine-readable and interoperable.  
Nevertheless, many applications, such as data integration, search, and interlinking, cannot take full advantage of Linked Data if it is of low quality.
There exist a few approaches for the quality assessment of Linked Data, but their performance degrades with the increase in data size and quickly grows beyond the capabilities of a single machine.
To answer question~\ref{rqc:2}, in this thesis, we present DistQualityAssessment (cf.\ Chapter~\ref{chapter:dist_quality_assessment} -- an open source implementation of quality assessment of large \gls{RDF} datasets that can scale out to a cluster of machines.
This is the first distributed, in-memory approach for computing different quality metrics for large \gls{RDF} datasets using Apache Spark. We also provide a quality assessment pattern that can be used to generate new scalable metrics that can be applied to big data.
The work presented here is integrated with the SANSA framework and has been applied to at least three use cases beyond the SANSA community.   
The results show that our approach is more generic, efficient, and scalable as compared to previously proposed approaches.
Overall, we provide the following contributions to the state-of-the-art:

\begin{itemize}
    \item We present a Quality Assessment Pattern $\mathcal{QAP}$ to characterize scalable quality metrics.
    \item We provide DistQualityAssessment -- a distributed (open source) implementation of quality metrics using Apache Spark.
    \item We performed an analysis of the complexity of the metric evaluation in the cluster.
    \item We evaluate our approach and demonstrate empirically its superiority over a previous centralized approach.
    \item We integrated the approach into the SANSA framework.
    SANSA is actively maintained and uses the community ecosystem (mailing list, issues trackers, continuous integration, website, etc.).
\end{itemize}

The third problem we tackled in this thesis was the problem of querying and retrieving distributed \gls{RDF} datasets in an efficient and effective way and answers the following research question.

\begin{tcolorbox}
\textbf{\rqNr[RQ3]\label{rqc:3}}: Can distributed \gls{RDF} datasets be queried efficiently and effectively?
\end{tcolorbox}

One of the key features of Big Data is its complexity in terms of representation, structure, or formats.
One existing way to deal with it is offered by Semantic Web standards.
Among them, \gls{RDF} --which proposes to model data with triples representing edges in a graph-- has received a large success and the semantically annotated data has grown steadily towards a massive scale.
Therefore, there is a need for scalable and efficient query engines capable of retrieving such information.
To answer~\ref{rqc:3}, in Chapter~\ref{chapter:scalable_rdf_querying} we proposed scalable approaches for \gls{SPARQL} query evaluation over distributed \gls{RDF} data. 
First, Sparklify -- a scalable software component for efficient evaluation of \gls{SPARQL} queries over distributed \gls{RDF} datasets. 
It uses Sparqlify as a SPARQL-to-SQL rewriter for translating SPARQL queries into Spark executable code.
Our preliminary results demonstrate that our approach is more extensible, efficient, and scalable as compared to state-of-the-art approaches.
As a second approach, we investigated and implemented a scalable semantic-based query engine for efficient evaluation of \gls{SPARQL} queries over distributed \gls{RDF} datasets. 
It uses a semantic-based partitioning strategy as the data distribution and converts \gls{SPARQL} to Spark executable code.
We have shown empirically that a semantic-based approach can scale horizontally and perform well as compared with the previous Hadoop-based system: the SHARD triple store.
It is also comparable with other in-memory \gls{SPARQL} query evaluators when there is less shuffling involved i.e. less duplicate values.
Both approaches are integrated into a larger SANSA framework and Sparklify serves as a default query engine and has been used by at least three external use scenarios.
Overall, we provide the following contributions to the state-of-the-art:

\begin{itemize}
 \item We present a novel approach for vertical partitioning including \gls{RDF} terms using the distributed computing framework, Apache Spark.
 \item We developed a scalable query system using Sparqlify -- a SPARQL-to-SQL rewriter on top of Apache Spark.
 \item We evaluated Sparklify with state-of-the-art engines and demonstrate it empirically.
 \item A scalable approach for semantic-based partitioning using the distributed computing framework, Apache Spark.
 \item A scalable semantic-based query engine (\textit{SANSA.Semantic}) on top of Apache Spark.
 \item Comparison of the semantic-based system with state-of-the-art engines and demonstrate the performance empirically.
 \item We integrated the proposed approaches into the SANSA larger framework.
 Sparklify serves as a default query engine in SANSA.
 SANSA is an active project and maintained, including issue tracker, mailing list, changelogs, website, etc.
\end{itemize}

%Last but not least, the question we want to answer in the scope of the thesis is the following:

%\begin{tcolorbox}
%\textbf{\rqNr[RQ4]\label{rqc:4}}: How can we exploit large-scale \gls{RDF} datasets for a particular use case and ensure scalability?
%\end{tcolorbox}

%With the increase of \gls{RDF} data, new challenges arise.
%With that in mind, new technologies need to be adaptable enough to work on different domain-specific applications.
%To answer~\ref{rqc:4}, we implemented and integrated all the proposed approaches throughout this thesis into a larger SANSA framework.
%Within SANSA, we have applied our approaches to many different use cases.
%On the scope of this thesis, we mention three of them (cf.\ Chapter~\ref{chapter:implementation_and_use_cases}).
%For the Ethereum analysis use case, we show some of the potential use cases of scalable analysis of large Ethereum \gls{RDF} data, i.e. the Hub and Authorities transaction network analysis, and Crypto Kitties analysis.
%During the development and evaluation of this use case, we were able to show and demonstrate the feasibility and scalability of our approach when dealing with a large amount of data.
%For the Big Data logs mining, we show how our approaches can be used to mine different logs generated by the stack of Big Data components for further analysis such as root cause analysis.
%Lastly, in the geospatial domain, we show how the scalable integration of big \gls{POI} data can be used for better hints about \gls{AOI}s within the region.

\section{Limitations and Future Directions}
In this section, we discuss the limitations we identified during this study and potential future directions to take in order to overcome such limitations.

In the following, we summarize the limitations and future directions on each of the main contributions of this thesis.

\begin{itemize}
    \item \textit{Large-scale \gls{RDF} Dataset Statistics} --
    In Chapter~\ref{chapter:dist_lod_stats} we have demonstrated that our approach is scalable when computing statistics over a large amount of \gls{RDF} data as compared with a centralized approach.
    Nevertheless, we plan to further improve time efficiency by persisting the data to an even higher extent more in memory and perform load balancing, which could further improve the performance.
    Moreover, as our implementation is purely batch processing, in which the data chunks are normally very large we plan to investigate additional techniques for lowering the network overhead and I/O footprint.
    In this regard, efficient compression (e.g. \gls{HDT}~\cite{HDTFernandez2013}) methods for lowering data communication would be very relevant.
    Finally, as our main focus is on applying distributed techniques to \gls{RDF} data processing, we plan to port the existing solution for near real-time computation of \gls{RDF} dataset statistics.
    
    \item \textit{Assessment of \gls{RDF} Datasets at Scale} -- 
    Although we have achieved reasonable results in terms of scalability (cf.\ Chapter~\ref{chapter:dist_quality_assessment}), we plan to further improve time efficiency by applying intelligent partitioning strategies and persist the data to an even higher extent in memory and perform dependency analysis in order to evaluate multiple metrics simultaneously. 
    We also plan to explore near real-time interactive quality assessment of large-scale \gls{RDF} data using Spark Streaming.
    Finally, in the future, we intend to develop a declarative plugin for the current work using Quality Metric Language (QML)~\cite{debattista2016luzzu}, which gives users the ability to express, customize and enhance quality metrics.
    Besides the above mentioned future direction, as a long-term vision, we plan to offer DistQualityAssessment as a Service.
    It is obvious that the quality assessment of \gls{RDF} is not considered a one-off event but, on the contrary, intends to be constantly evolving.
    Therefore, these changes have to be reflected as well.
    However, given the large-scale of such \gls{RDF} datasets, one should consider various strategies for crawling and assessing the quality of the data.
    Currently, there is a number of crawlers available, such as the LODStats\furl{http://lodstats.aksw.org/} project, which has crawled \gls{RDF} data from metadata portals for the past eight years. 
    It interacts with the CKAN dataset metadata registry to obtain a comprehensive picture of the current state of the Data Web.
    While crawling the data, and specifically over large-scale \gls{RDF} datasets, data quality check is a must.
    The current solution does not provide such option, therefore, integration of our approach with the LODStats project could bring another view w.r.t to the quality of the data
    
    \item \textit{Scalable \gls{RDF} Querying} -- 
    In this thesis, we showed that the application of OBDA tooling to Big Data frameworks achieves promising results in terms of scalability. 
    We present a working prototype implementation that can serve as a baseline for further research. 
    Our next steps include evaluating other tools, such as Ontop~\cite{Calvanese2017OntopAS}, and analyze how their performance in the Big Data setting can be improved further. 
    For example, we intend to investigate how OBDA tools can be combined with dictionary encoding of \gls{RDF} terms as integers and evaluate the effects.
    In addition to that, we plan to further extend our parser to support more \gls{SPARQL} fragments and adding statistics to the query engine while evaluating queries. 
    We want to analyze the query performance in the large-scale \gls{RDF} datasets and explore prospects for improvement.
    For example, we intend to investigate the re-ordering of the \gls{BGP}s and evaluate the effects on query execution time.
    In this regard, efficient strategies, as well as a detailed cost function for query plan optimization, have to be considered.
    In addition, we also plan to consider other data management operations i.e. additions, updates, deletions and materialization of the results.
    One solution could be considering the Delta\furl{https://delta.io/} lake solution as an alternative for storage layer that brings ACID transactions to \gls{RDF} data management solutions.
\end{itemize}

In addition to the future work mentioned above, we see a potential future direction as a long term vision of this work, in an
attempt to foster the interest in scalable processing of \gls{RDF} datasets.

\begin{itemize} 
  \item \textit{Adaptive Distributed \gls{RDF} Querying} -- 
  Often the power of freedom while designing \gls{SPARQL} queries leads to very complex and performance deficits in \gls{SPARQL} query evaluation.
  Within our \gls{SPARQL} query evaluators, we will go beyond that by developing adaptive data distribution strategies, that generate and optimize index structures and distribute data based on anticipated query workloads of particular inference or machine learning algorithms.
  
  \item \textit{Efficient Recommendation System for \gls{RDF} Partitioners} -- 
  In order to store and query big \gls{RDF} datasets efficiently in distributed environments, different partitioning techniques need to be implemented. 
  Several techniques have been proposed for splitting Big \gls{RDF} Data, ranging from vertical (cf. Section~\ref{sec:sparklify-approach}), hash, graph to semantic-based (cf. Section~\ref{sec:semantic-based-approach}) partitioners. 
  However, the selection of the “best partitioner” depends highly on the structure of the dataset and the query efficiency and effectiveness are coupled to the query engine used. 
  We aim to develop a recommender system that will suggest the “best partitioner” for both of our \gls{SPARQL} query evaluators based on the structure of the data gathered from DistLODStats (cf. Chapter~\ref{chapter:dist_lod_stats}) and specific requirements.
  
  \item \textit{A Powerful Benchmarking Suite} -- 
  In order to decide which distributed \gls{SPARQL} query evaluator performs best for specific query loads over a large-scale \gls{RDF} dataset, it is required to perform benchmarks. 
  Benchmarking is an extremely tedious task demanding repetitive manual effort, therefore it is required to automate the whole process.
  However, there are currently no benchmarking frameworks that support benchmarking and comparing diverse distributed \gls{SPARQL} query evaluators.
  To this end, we will make use of the existing benchmarking platform i.e. LITMUS~\cite{ThakkarEtAl:LITMUS16}, HOBBIT~\cite{Roeder2019Hobbit} and extend them toward supporting distributed settings.
\end{itemize}

\section{Closing Remarks}
With the increasing amount of the \gls{RDF} data, processing large-scale \gls{RDF} datasets are constantly facing challenges and a lot of potential for exploration.
During this thesis, we have shown the benefits of distributed computing frameworks to successfully tackle the problem of scalable and efficient processing of \gls{RDF} datasets.
More specifically, we have presented the details of three core components: 1) scalable \gls{RDF} dataset statistics evaluation, 2) distributed quality assessment of large amounts of \gls{RDF} data, and 3) efficient and scalable \gls{SPARQL} query evaluators.
In addition, we have shown the usage of the proposed techniques into real-world use cases.
Future research work can build upon the contributions presented during this thesis as a starting point for a comprehensive and out-of-the-box scalable processing of large-scale \gls{RDF} datasets.
The main contributions of this thesis have been integrated within the SANSA framework and are making an impact on the semantic web community and several semantic web applications in the big data era -- resulting in a SANSA framework and being used in many European research projects.


