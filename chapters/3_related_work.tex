%==============================================================================
\chapter{Related Work}
\label{chapter:related}
%==============================================================================

This chapter reviews the related work to our research, according to the research problem and research questions defined in Chapter~\ref{chapter:intro}.
We first discuss and compare the state-of-the-art \gls{RDF} dataset statistics systems.
Then, we give an overview and discuss previous work related to \gls{RDF} quality assessment frameworks. 
Finally, we cover existing \gls{SPARQL} query evaluators and position our proposed solutions. 

This chapter is based on the related work sections from following publications~\cite{sejdiu-2018-dist-lod-stats-iswc,sejdiu-2019-sansa-dist-quality-assessment-iswc,sejdiu-2019-sansa-semantic-based-semantics, 2019-sansa-sparklify-iswc}:

\begin{itemize}
    \item \textbf{Gezim Sejdiu}, Ivan Ermilov, Jens Lehmann, and Mohamed Nadjib-Mami. “\href{http://jens-lehmann.org/files/2018/iswc_distlodstats.pdf}{DistLODStats: Distributed Computation of RDF Dataset Statistics},” in Proceedings of 17th International Semantic Web Conference (ISWC), 2018.
    
    \item \textbf{Gezim Sejdiu}, Anisa Rula, Jens Lehmann, and Hajira Jabeen. “\href{http://jens-lehmann.org/files/2019/iswc_dist_quality_assessment.pdf}{A Scalable Framework for Quality Assessment of RDF Datasets},” in Proceedings of 18th International Semantic Web Conference (ISWC), 2019.
    
    \item \textbf{Gezim Sejdiu}, Damien Graux, Imran Khan, Ioanna Lytra; Hajira Jabeen; and Jens Lehmann. “\href{https://gezimsejdiu.github.io/publications/semantic_based_query_paper_SEMANTICS2019.pdf}{Towards A Scalable Semantic-based Distributed Approach for SPARQL query evaluation},” 15th International Conference on Semantic Systems (SEMANTiCS), Research \& Innovation , 2019.
     
    \item Claus Stadler, \textbf{Gezim Sejdiu}, Damien Graux, and Jens Lehmann. “\href{http://jens-lehmann.org/files/2019/iswc_sparklify.pdf}{Sparklify: A Scalable Software Component for Efficient evaluation of SPARQL queries over distributed RDF datasets},” in Proceedings of 18th International Semantic Web Conference (ISWC), 2019. 
    
\end{itemize}

\section{RDF Dataset Statistics Systems}
In this section, we provide an overview of related work regarding \gls{RDF} dataset statistics calculation.

To the best of our knowledge, all but one existing approaches use small to medium scale datasets and do not horizontally scale.

A dataset is large-scale w.r.t. a particular task in the scope of this article if the main memory on commodity hardware is insufficient to perform the task (without swapping to disk). 
We mention here, for example 
RDFStats~\cite{conf/dexaw/LangeggerW09}, which is a framework for generating statistics from \gls{RDF} data that can be used for \gls{SPARQL} query optimization while processing \gls{RDF} data over \gls{SPARQL} endpoints.
Such statistics includes histograms about subjects (\gls{URI}s, and blank nodes) and histograms about properties and corresponding ranges.
The tool can be integrated into user interfaces and other applications that utilize the Jena toolkit in order to provide such statistics for better performance when processing \gls{RDF} data.
But, the main purpose of the tool is to collect statistics for query optimization rather than generating VoID~\cite{Zhao:11:VoID}.

RDF$_{Pro}$~\cite{SAC-2015-CorcoglionitiRM} offers a suite of stream-oriented, highly optimized processors for common tasks, such as data filtering, RDFS inference, smushing, as well as statistics extraction.
The main component of the tool is a so-called \textit{\gls{RDF} processor}, a Java component which consumes an input stream of \gls{RDF} quads containing \gls{RDF} triples with an optional fourth named graph component in one or more passes.
It does by downloading and filtering the desired \gls{RDF} quads and place them into a separate graphs in order to track the provenance.
A metadata file is added as a link between each graph generated during the process, to the \gls{URI} of the associated sources (e.g. DBpedia).
Afterwords, it extracts the TBox information from such filtered data and then sorts them. 
The consequence step drop unnecessary top level classes and vocabulary alignments.
The process follow the smushing step -- using of canonical \gls{URI}s for each $\verb|owl:sameAs|$ equivalence class, producing an intermediate results (file) containing smushed data.
The inference of smushed data is computed and saved.
These intermediate results contains duplicate data, e.g. the same subject, predicate and object.
RDF$_{Pro}$ does a deduplication process, by removing such duplicates.
Finally, \gls{RDF} dataset statistics are extracted and merged with the TBox data.

ExpLOD~\cite{KhatchadourianExpLOD2010} explores summaries of \gls{RDF} usage and interlinking among datasets. 
These summaries include information about the structure of the \gls{RDF} graph, such as the instantiated \gls{RDF} classes of a resource or the property usage.
The tool also provides statistics about the number of corresponding entities connected using the $\verb|owl:sameAs|$ predicate to describe the interlinking between datasets.
The tool can also produce \gls{SPARQL} queries from a summary.

ProLOD~\cite{Bhm2010ProfilingLO} is a web-based profiling tool, with a possibility to analyze \gls{RDF} data and thus provide a deeper understanding of the underlying structure and semantics. 
It analyzes the object values of RDF triples and generates statistics upon them such as data type and pattern distribution.
ProLOD++~\cite{Abedjan2014ProfilingAM} which is an extension of ProLOD is also a browser-based tool that implements several algorithms with the aim to compute different profiling, mining or cleansing tasks. In the profiling, the task includes features like finding frequencies and distribution of distinct subjects, predicates, and objects, range of the predicates, string pattern analysis, link analysis and data type analysis.
\fixme{rephrase}

Another related approach we are aware of is Aether~\cite{makela2014aether}, which is an application for generating, viewing and comparing extended VoID statistical descriptions of \gls{RDF} datasets.
The tool is useful, for example, in getting to know a newly encountered dataset, in comparing the different versions of a dataset, and in detecting outliers and errors.
By giving a \gls{SPARQL} endpoint, the Aether tool can generate an extended VoID description containing a wide variety of characteristics describing the dataset.
Later, these statistics can then be viewed in order to get a better overview of the dataset.
The viewer component of the Aether can be also useful on comparing dataset descriptions to each other, so that the changes between two different versions of the dataset can be captured.

However, only one work we came across that provided a distributed framework for \gls{RDF} statistics computation: \textbf{LODOP}~\cite{Forchhammer:PROFILES:14}. 
LODOP adopts a MapReduce approach for computing, optimizing, and benchmarking data profiling techniques.
It uses Apache Pig as the underlying computation engine (Hadoop-based). 
LODOP implements 15 data profiling tasks comparing to 32 in our work. 
Because of the usage of MapReduce, the framework has a significant drawback: materialization of intermediate results between Map and Reduce and between two subsequent jobs is done on disk.
DistLODStats does not use the disk-based MapReduce framework (Hadoop), but rather bases its computation mainly in-memory, so runtime performance is presumably better~\cite{Shi:2015:CTM:2831360.2831365}.
Unfortunately, we were unable to run LODOP for comparison. This is due to technical problems encountered, despite the very significant effort we devoted to deploy and run it.

To the best of our knowledge, DistLODStats is the first software component for in-memory distributed computation of \gls{RDF} dataset statistics. 

\section{RDF Quality Assessment Frameworks}
Even though quality assessment of big datasets is an important research area, it is still largely under-explored. 
There have been a few works discussing the challenges and issues of big data quality~\cite{becker2015big,RaoG015,cai2015challenges}. 
Only recently, a few of them have started to address the problem from a practical point of view~\cite{debattista2016luzzu}, which is the focus of our work w.r.t the quality assessment of \gls{RDF} datasets.
In the following, we divide the section between conceptual and practical approaches proposed in the state of the art for big data quality assessment.

In~\cite{CatarciSCD17} the authors propose a big data processing pipeline and a big data quality pipeline. 
For each of the phases of the processing pipeline they discuss the corresponding phase of the big data quality pipeline.
Relevant quality dimensions such as accuracy, consistency and completeness are discussed for the quality assessment of \gls{RDF} datasets as part of an integration scenario.
Given that the quality dimensions and metrics have somehow evolved from relational to Linked Data,
it is relevant to understand the evolution of quality dimensions according to the differences between the structural characteristics of the two data models~\cite{BatiniRSV15}. 
This allows to manage the huge variability of methods and techniques needed to manage data quality and understand which are the quality dimensions that prevail when assessing large-scale \gls{RDF} datasets.

Most of the existing approaches can be applied to small/medium scale datasets and do not horizontally scale~\cite{debattista2016luzzu,KontokostasWAHLCZ14}. 
The work in~\cite{KontokostasWAHLCZ14} presents a methodology for assessing the quality of Linked Data based on a test case generation analogy used for software testing. 
The idea of this approach is to generate templates of the \gls{SPARQL} queries (i.e., quality test case patterns) and then instantiate them by using the vocabulary or schema information, thus producing quality test case queries. 
Luzzu~\cite{debattista2016luzzu} is similar in spirit with our approach in that its objective is to provide a framework for quality assessment.
Its Quality Metric Language (LQML), is a \gls{DSL} that enables knowledge engineers to declaratively define quality metrics whose definitions can be understood more easily. 
LQML offers notations, abstractions and expressive power, focusing on the representation of quality metrics.
In contrast to our approach, where data is distributed and also the evaluation of metrics is distributed, Luzzu does not provide any large-scale processing of the data. 
It only uses Spark streaming for loading the data which is not part of the core framework. 
Another approach proposed for assessing the quality of large-scale medical data implements Hadoop Map/Reduce~\cite{BonnerMKBTMCA15}. 
It takes advantage of query optimization and join strategies which are tailored to the structure of the data and the \gls{SPARQL} queries for that particular dataset. In addition, this work, differently from our approach, does not assess any data quality metric defined in~\cite{zaveri2015quality}.
The work in~\cite{BenbernouO17} propose a reasoning approach to derive inconsistency rules and implements a Spark-based implementation of the inference algorithm for capturing and cleaning inconsistencies in \gls{RDF} datasets. 
The inference generally incurs higher complexity. Our approach is designed for scalability, and we also use Spark-based implementation for capturing inconsistencies in the data.
While the approach in~\cite{BenbernouO17} needs manual definitions of the inconsistency rules, our approach runs automatically, not only for consistency metrics but also for other quality metrics. 
In addition, we test the performance of our approach on large-scale \gls{RDF} datasets while their approach is not experimentally evaluated. 
LD-Sniffer~\cite{Mihindukulasooriya2016LDSA}, is a tool for assessing the accessibility of Linked Data resources according to the metrics defined in the Linked Data Quality Model. 
The limitation of this tool, besides that it is a centralized version, is that it does not provide most of the quality assessment metrics defined in~\cite{zaveri2015quality}. 
In addition to above, there is a lack of unified structure to propose and develop new quality metrics that are scalable and less computationally expensive.

Based on the identified limitations of these aforementioned approaches, we have introduced DistQualityAssessment which bases its computation and evaluations mainly in-memory.
As a result the computation of the quality metrics show a high performance for large-scale datasets.

\section{SPARQL query evaluators}

\defn{Partitioning of RDF Data}
Centralized \gls{RDF} stores use relational (e.g., Sesame \cite{BroekstraKH02}), property (e.g., Jena~\cite{Wilkinson06}), or binary tables (e.g., SW-Store~\cite{AbadiMMH09}) for storing \gls{RDF} triples or maintain the graph structure of the \gls{RDF} data (e.g., gStore~\cite{ZouMCOZ11}). 
For dealing with big \gls{RDF} datasets, vertical partitioning and exhaustive indexing are commonly employed techniques. 
For instance, Abadi et al.~\cite{AbadiMMH07} introduce a vertical partitioning approach in which each predicate is mapped to a two-column table containing the subject and object. 
This approach has been extended in Hexastore~\cite{WeissKB08} to include all six permutations of subject, predicate, and object (s, p, o). 
To improve the efficiency of \gls{SPARQL} queries RDF-3X~\cite{NeumannW10} has adopted exhaustive indices not only for all (s, p, o) permutations but also for their binary and unary projections. 
While some of these techniques can be used in distributed configurations as well, storing and querying \gls{RDF} datasets in distributed environments pose new challenges such as the scalability. 
In our approach, we tackle partitioning and querying of big \gls{RDF} datasets in a distributed manner.

Partitioning-based approaches for distributed \gls{RDF} systems propose to partition an \gls{RDF} graph in fragments which are hosted in centralized \gls{RDF} stores at different sites. 
Such approaches use either standard partitioning algorithms like METIS~\cite{GurajadaSMT14} or introduce their own partitioning strategies. 
For instance, Lee et al.~\cite{LeeL2013} define a partition unit as a vertex with its closest neighbors based on heuristic rules while DiploCloud~\cite{WylotC16} and AdPart~\cite{harbi2016accelerating} use physiological \gls{RDF} partitioning based on \gls{RDF} molecules. 
In our proposal, we use both, vertical partitioning and semantic-based partitioning approaches.

\defn{Hadoop-based systems}
Cloud-based approaches for managing large-scale \gls{RDF} mainly use NoSQL distributed data stores or employ various partitioning approaches on top of Hadoop infrastructure, i.e., the \gls{HDFS} and its MapReduce implementation, in order to leverage computational resources of multiple nodes. 
For instance, Sempala~\cite{Schatzle2014Sempala} is a Hadoop-based approach which serves as SPARQL-to-SQL approach on top of Hadoop.
It uses Impala\furl{https://impala.apache.org/} as a distributed SQL processing engine. 
Sempala uses unified vertical partitioning based on a single property table to improve the runtime of the star-shaped queries by excluding the joins. 
The limitation of Sempala is that it was designed only for that particular shape of the queries.
PigSPARQL~\cite{Schatzle2011PMS} uses Hadoop based implementation of vertical partitioning for data representation. 
It translates \gls{SPARQL} queries into Pig\furl{https://pig.apache.org/} LATIN queries and runs them using the Pig engine.
A most recent approach based on MapReduce is RYA~\cite{Punnoose2012Rya}.
It is a Hadoop based scalable \gls{RDF} store which uses Accumulo\furl{accumulo.apache.org} as a distributed key-value store for indexing the \gls{RDF} triples.
One of RYA's advantages is the power of performing join reorder. 
The main drawback of RYA is that it relies on disk-based processing increasing query execution times.
Other \gls{RDF} systems like JenaHBase~\cite{KhadilkarKTC2012} and H2RDF+~\cite{PapailiouKTKK13} use the Hadoop database HBase for storing triple and property tables.

SHARD~\cite{Rohloff2010SHARD} is one approach which groups \gls{RDF} data into a dedicated partition so-called semantic-based partition.  
It groups these \gls{RDF} data by subject and implements a query engine which iterates through each of the clauses used on the query and performs a query processing. 
A MapReduce job is created while scanning each of the triple patterns and generates a single plan for each of the triple pattern which leads to a larger query plan, therefore, it contains too many Map and Reduces jobs. 
Our partitioning algorithm implemented on Semantic-based query engine is based on SHARD, but instead of creating MapReduce jobs we employ the Spark framework in order to increase scalability.

\defn{In-Memory systems}
S2RDF~\cite{Schatzle:2016:SRQ:2977797.2977806} is a distributed query engine which translates \gls{SPARQL} queries into SQL ones while running them on Spark-SQL. 
It introduces a data partitioning strategy that extends vertical partitioning with additional statistics, containing pre-computed semi-joins for query optimization.
SPARQLGX~\cite{sparqlgx-iswc-2016} is similar to S2RDF, but instead of translating \gls{SPARQL} to SQL, it maps \gls{SPARQL} into direct Spark \gls{RDD} operations. 
It is a scalable query engine which is capable of evaluating efficiently the \gls{SPARQL} queries over distributed \gls{RDF} datasets~\cite{graux2018multi}.
It uses a simplified VP approach, where each predicate is assigned to a specific parquet file. 
As an addition, it is able to assign \gls{RDF} statistics for further query optimization while also providing the possibility of directly query files on the \gls{HDFS} using SDE.
